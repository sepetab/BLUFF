{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d0f2038",
   "metadata": {},
   "source": [
    "##### Implementation was inspired from this [notebook](https://www.kaggle.com/code/madz2000/sarcasm-detection-with-glove-word2vec-83-accuracy) at kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f588f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import tensorflow\n",
    "import tensorflow.keras as keras\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "import re,string,unicodedata\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from string import punctuation\n",
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,GRU\n",
    "import gensim\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "415c81fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "train_data = pd.read_csv(\"../Data/train.csv\")\n",
    "test_data = pd.read_csv(\"../Data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea2b8595",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parse dataset -- In total : 28619 samples\n",
    "\n",
    "# get stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "punctuation = list(string.punctuation)\n",
    "stop.update(punctuation)\n",
    "\n",
    "def strip_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "\n",
    "#Removing the square brackets\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "# Removing URL's\n",
    "def remove_between_square_brackets(text):\n",
    "    return re.sub(r'http\\S+', '', text)\n",
    "\n",
    "#Removing the stopwords from text\n",
    "def remove_stopwords(text):\n",
    "    final_text = []\n",
    "    for i in text.split():\n",
    "        if i.strip().lower() not in stop:\n",
    "            final_text.append(i.strip())\n",
    "    return \" \".join(final_text)\n",
    "\n",
    "#Removing the noisy text\n",
    "def denoise_text(text):\n",
    "    text = strip_html(text)\n",
    "    text = remove_between_square_brackets(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text\n",
    "\n",
    "#Apply function on headline column\n",
    "train_data['headline']=train_data['headline'].apply(denoise_text)\n",
    "test_data['headline']=test_data['headline'].apply(denoise_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b6ccba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets words by splitting sentences\n",
    "def get_words(sentences=pd.concat([train_data['headline'], test_data['headline']], ignore_index=True)):\n",
    "    words = []\n",
    "    for i in sentences.values:\n",
    "        words.append(i.split())\n",
    "    return words\n",
    "\n",
    "\n",
    "#Dimension to embed the words\n",
    "EMBEDDING_DIM = 200\n",
    "\n",
    "#Creating Word Vectors by Word2Vec Method (takes time...)\n",
    "w2v_model = gensim.models.Word2Vec(sentences = get_words() , vector_size=EMBEDDING_DIM , window = 5 , min_count = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5020e8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer \n",
    "def tokenize(words = get_words()):\n",
    "    tokenizer = text.Tokenizer(num_words=35000)\n",
    "    tokenizer.fit_on_texts(words)\n",
    "    tokenized_train = tokenizer.texts_to_sequences(words)\n",
    "    x = sequence.pad_sequences(tokenized_train, maxlen = 20)\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "    return (tokenizer,x,vocab_size)\n",
    "\n",
    "#Get back train and test data\n",
    "tokenizer,x,vocab_size = tokenize()\n",
    "x_train = x[:len(train_data),:]\n",
    "x_test = x[len(train_data):(len(train_data)+len(test_data)),:]\n",
    "y_train = train_data['is_sarcastic'].tolist()\n",
    "y_test = test_data['is_sarcastic'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07f23e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create weight matrix from word2vec gensim model\n",
    "def get_weight_matrix(model, vocab):\n",
    "    # total vocabulary size plus 0 for unknown words\n",
    "    vocab_size = len(vocab) + 1\n",
    "    # define weight matrix dimensions with all 0\n",
    "    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
    "    # step vocab, store vectors using the Tokenizer's integer mapping\n",
    "    for word, i in vocab.items():\n",
    "        weight_matrix[i] = model.wv[word]\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d7327c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer\n",
    "embedding_vectors = get_weight_matrix(w2v_model, tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c298237a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tryptophanv2/opt/anaconda3/envs/elmo/lib/python3.6/site-packages/tensorflow_core/python/keras/initializers.py:119: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/tryptophanv2/opt/anaconda3/envs/elmo/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/tryptophanv2/opt/anaconda3/envs/elmo/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/tryptophanv2/opt/anaconda3/envs/elmo/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/tryptophanv2/opt/anaconda3/envs/elmo/lib/python3.6/site-packages/tensorflow_core/python/ops/init_ops.py:97: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/tryptophanv2/opt/anaconda3/envs/elmo/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "#Defining Neural Network\n",
    "model = Sequential()\n",
    "#Non-trainable embeddidng layer\n",
    "model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=20, trainable=True))\n",
    "#LSTM \n",
    "model.add(Bidirectional(LSTM(units=128 , recurrent_dropout = 0.3 , dropout = 0.3,return_sequences = True)))\n",
    "model.add(Bidirectional(GRU(units=32 , recurrent_dropout = 0.1 , dropout = 0.1)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr = 0.01), loss='binary_crossentropy', metrics=['acc'])\n",
    "\n",
    "del embedding_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ed615c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, 20, 200)           7614400   \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 20, 256)           336896    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                55488     \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 8,006,849\n",
      "Trainable params: 8,006,849\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f70ffe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 21464 samples, validate on 7155 samples\n",
      "Epoch 1/3\n",
      "21464/21464 [==============================] - 200s 9ms/sample - loss: 0.2341 - acc: 0.9080 - val_loss: 0.2573 - val_acc: 0.8935\n",
      "Epoch 2/3\n",
      "21464/21464 [==============================] - 197s 9ms/sample - loss: 0.0499 - acc: 0.9827 - val_loss: 0.3702 - val_acc: 0.8861\n",
      "Epoch 3/3\n",
      "21464/21464 [==============================] - 198s 9ms/sample - loss: 0.0162 - acc: 0.9952 - val_loss: 0.4839 - val_acc: 0.8756\n"
     ]
    }
   ],
   "source": [
    "# Fit model on trian and test data\n",
    "history = model.fit(x_train, y_train, batch_size = 128 , validation_data = (x_test,y_test) , epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0a9eac9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get prediction probabilities\n",
    "pred_probs = model.predict_proba(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f25c0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump probabilities as a pickle file\n",
    "def dump_probs(pred_probs = pred_probs):\n",
    "    with open('./probs_for_1.pkl','wb') as f:\n",
    "        pickle.dump(pred_probs,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db5e88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from pickle file\n",
    "with open('./probs_for_1.pkl','rb') as f:\n",
    "    preds = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e0b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get labels from probabilities\n",
    "labels = [1 if x > 0.5 else 0 for x in preds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9041967b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8756114605171209"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get accuracy score\n",
    "accuracy_score(labels,test_data['is_sarcastic'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
